from pyspark.sql import SparkSession 
from pyspark.sql.types import StructType, StructField 
from pyspark.sql.types import DoubleType, IntegerType, StringType,FloatType 
schema = StructType([StructField("S_Application_Category", StringType()),StructField("S_Division", StringType()),StructField("S_Division_Type", StringType()),StructField("S_Calendar_month", StringType()),StructField("S_year", StringType()),StructField("S_Region", StringType()),StructField("S_CompanyCode", StringType()),StructField("S_Plant", StringType()),StructField("S_Plant_name", StringType()),StructField("S_KPI", StringType()),StructField("S_Notification", StringType()),StructField("S_material", StringType()),StructField("S_Material_name", StringType()),StructField("N_Return_Order_Qty", StringType()),StructField("S_Return_Order_Value", StringType()),StructField("N_Price", FloatType()),StructField("N_Return_Delivery_Qty", FloatType()),StructField("N_Return_Delivery_Value", FloatType()),StructField("N_COPQ_QTY", FloatType()),StructField("N_COPQ_Value", FloatType()),StructField("N_Redispatch_Qty", FloatType()),StructField("N_Available_Stock_Qty", FloatType()),StructField("N_ReWork_Input_excl_Scrap", FloatType()),StructField("N_Material_Transfer_Qty", FloatType()),StructField("N_Remaining_Qty", FloatType()),StructField("N_Scrap_Write_Off_Qty", FloatType()),StructField("N_BlockStock", FloatType()),StructField("N_Quality_Insp_Stock", FloatType()),StructField("N_ReWork_Scrap", FloatType()),StructField("N_No_Ret_Delv_Qty", FloatType()),StructField("N_COPQ_1", FloatType()),StructField("N_COPQ_2", FloatType())]) 
spark = SparkSession.builder.appName("Copq").config("spark.some.config.option", "sale").getOrCreate() 
df = spark.read.csv("D:/Shikha_working_directory/bi-vertib/datasource/csvFiles/COPQ.csv",header=True,mode="DROPMALFORMED",sep=",",schema=schema) 
df.write.format("jdbc").options(url="jdbc:mysql://192.168.10.63:3306/epinsight",driver="com.mysql.cj.jdbc.Driver",dbtable="tbl_copq_data",user="root",password="Admin@123").mode("overwrite").save() 
