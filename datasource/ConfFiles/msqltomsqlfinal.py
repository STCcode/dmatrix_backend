from pyspark.sql import SparkSession 
spark = SparkSession.builder.appName("msqltomsqlfinal").config("spark.some.config.option", "sale").getOrCreate() 
jdbcDF = spark.read.format("jdbc").options(url="jdbc:mysql://192.168.10.63:3306/ams_ph_2",driver="com.mysql.cj.jdbc.Driver",query="select Link as Link1,`Global Dimension 2 Code` as `Global Dimension 2 Code1`,`Entry Type` as `Entry Type1`,`Document No_` as `Document No_1`,Description as Description1,value as value,Source Code as Source Code,QTY as QTY,BEZ as BEZ,Posting Date as Posting Date,System as System,Gen_ Bus_ Posting Group as Gen_ Bus_ Posting Group,Gen_ Prod_ Posting Group as Gen_ Prod_ Posting Group,Source Posting Group as Source Posting Group,Item Ledger Entry Type as Item Ledger Entry Type,User ID as User ID,Dimension Set ID as Dimension Set ID,MFR Line as MFR Line,KstSt as KstSt,Contr/OPC as Contr/OPC,Posted at as Posted at,Mon as Mon,Jahr as Jahr,Cognos Cust as Cognos Cust,Segment as Segment,Diameter as Diameter,Hot_foil as Hot_foil,IBL as IBL,Insert as Insert,Shoulder_Colour as Shoulder_Colour,Shoulder_Type_Thread as Shoulder_Type_Thread,Silk_screening as Silk_screening,Top_Seal as Top_Seal,Tube_Length as Tube_Length,Web Type as Web Type,Entry No_ as Entry No_ from tbl_germany_sale",user="root",password="Admin@123").load() 
jdbcDF.write.format("jdbc").options(url="jdbc:mysql://127.0.0.1:3306/db_demo",driver="com.mysql.cj.jdbc.Driver",dbtable="tbl_gsale",user="root",password="Admin@123").mode("overwrite").save() 
